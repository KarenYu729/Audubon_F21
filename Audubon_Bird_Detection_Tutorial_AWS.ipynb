{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwP9RpOZ21zr"
   },
   "source": [
    "# AWS bird detection\n",
    "\n",
    "We thank F21 semesters student's for the base code\n",
    "\n",
    "Authors: Wenbin Li\n",
    "\n",
    "This jupyter notebook is meant to run on AWS sagemaker studio. All the file location should be on S3 bucket or any other bucket service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yj5EpyIO21Fp"
   },
   "source": [
    "## Installation and setup for Colab\n",
    "\n",
    "Run the next cells to setup Colab with the necessary requirements. We clone the Github repo with the developed code, and install dependencies, namely Detectron2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.6/site-packages (0.17.2)\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image) (2.2)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.6/site-packages (from scikit-image) (2020.9.3)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image) (3.3.4)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from scikit-image) (1.2.2)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image) (8.3.2)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image) (2.13.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.6/site-packages (from scikit-image) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.15.1 in /opt/conda/lib/python3.6/site-packages (from scikit-image) (1.19.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.11.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (3.0.6)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from networkx>=2.0->scikit-image) (5.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3PFNsJ_Tek3A"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:12: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# Import useful libraries\n",
    "import os, sys, shutil, glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import cv2\n",
    "from skimage import io  \n",
    "from datetime import datetime\n",
    "from distutils.dir_util import copy_tree\n",
    "import boto3\n",
    "import seaborn as sns\n",
    "from tqdm.autonotebook import tqdm\n",
    "from PIL import Image, ImageDraw\n",
    "import csv\n",
    "import random\n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZK9R5DgH2jBr"
   },
   "outputs": [],
   "source": [
    "# This cell only excecutes if you're running on Colab. \n",
    "# if 'google.colab' in sys.modules:\n",
    "#   from google.colab import drive \n",
    "#   drive.mount('/gdrive/') # Mount Google Drive! \n",
    "\n",
    "  # Clone Audubon bird detection Github repo \n",
    "#   !git clone https://github.com/RiceD2KLab/Audubon_F21.git \n",
    "\n",
    "  # Install dependencies \n",
    "  !pip install -qq pyyaml==5.1\n",
    "  # This is the current pytorch version on Colab. Uncomment this if Colab changes its pytorch version\n",
    "  !pip install -qq torch==1.9.0+cu102 torchvision==0.10.0+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "  # Install detectron2 that matches the above pytorch version\n",
    "  # See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
    "  !pip install -qq detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.9/index.html\n",
    "  # exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime\n",
    "\n",
    "  !pip install -qq wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHHm2ikxC99D"
   },
   "source": [
    "## Data exploration & wrangling\n",
    "\n",
    "The following cells contain the data exploration and wrangling modules of the data science pipeline. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9m81HuKnD-J8"
   },
   "source": [
    "### Load dataset from Google Drive \n",
    "\n",
    "The following cell unzips a folder stored on Google Drive ontop the Colab machine. You can modify this cell to load your drone images onto the Colab instance! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 = boto3.resource('s3')\n",
    "# for bucket in s3.buckets.all():\n",
    "#     print(bucket.name)\n",
    "    \n",
    "# my_bucket = 'sagemaker-studio-zd1j05seaof'\n",
    "# my_file = '1017_1/DJI_20210520122305_0032.JPG'\n",
    "# s3client = boto3.client('s3')\n",
    "# response = s3client.get_object(Bucket=my_bucket, Key=my_file)\n",
    "# body = response['Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# from io import BytesIO\n",
    "\n",
    "# im = Image.open(body)\n",
    "# image = np.array(im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (20,10))\n",
    "# plt.imshow(image[500:1300, 500:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to load up the file path for the annotation\n",
    "my_bucket = 'sagemaker-studio-zd1j05seaof'\n",
    "conn = boto3.client('s3')\n",
    "annot_path = 'annotation_1017/'\n",
    "annot = conn.list_objects_v2(Bucket=my_bucket, Prefix=annot_path)['Contents']\n",
    "# for f in annot:\n",
    "# #     print(f['Key']) # print what files are in this specific folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aa = [x['Key'] for x in conn.list_objects_v2(Bucket=my_bucket, Prefix=annot_path)['Contents'] if x['Key'].split('.')[-1] == 'bbx']\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aa[1] = 'annotation_1017/DJI_20210520122304_0031.bbx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  d = os.path.basename('./annotation_1017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annot_path.split('/')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aat = s3client.get_object(Bucket = my_bucket, Key = aa[0])\n",
    "# # pd.read_csv(aat['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.Random(4).shuffle(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3client.put_object(Bucket = my_bucket, Key = 'test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_source = {'Bucket': my_bucket, 'Key': aa[1]}\n",
    "\n",
    "# s3 = boto3.resource('s3')\n",
    "\n",
    "# s3.meta.client.copy(copy_source, my_bucket, 'test/'+aa[1].split('/')[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yA32BBX_JX-r"
   },
   "source": [
    "### Data exploration on AWS\n",
    "\n",
    "The following cells generate some metrics and plots to help understand the loaded dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3client = boto3.client('s3')\n",
    "target_data = []\n",
    "for f in annot:\n",
    "    response = s3client.get_object(Bucket=my_bucket, Key=f['Key'])\n",
    "    body = response['Body']\n",
    "    target_data.append(pd.read_csv(body, header=0, \n",
    "                       names = [\"class_id\", \"class_name\", \"x\", \"y\", \"width\", \"height\"] ))\n",
    "target_data = pd.concat(target_data, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Bird Species Distribution\n",
      "Laughing Gull Adult       772\n",
      "Mixed Tern Adult          286\n",
      "Mixed Tern Flying          19\n",
      "Laughing Gull Flying       17\n",
      "Other Bird                 16\n",
      "Trash/Debris               13\n",
      "Brown Pelican Adult         5\n",
      "Brown Pelican Juvenile      1\n",
      "Name: class_name, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# target_data\n",
    "print('\\n Bird Species Distribution')\n",
    "print(target_data[\"class_name\"].value_counts())\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = target_data[\"class_name\"].value_counts().plot.bar(figsize=(10,6))  \n",
    "ax.set_title('Bird Species Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_exist(my_bucket, 'annotation_1017/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5460, 8192, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from PIL import Image \n",
    "from PIL import Image\n",
    "from utils import plotting\n",
    "from utils.cropping import csv_to_dict_AWS\n",
    "\n",
    "annot_dict = csv_to_dict_AWS(bucket_name= my_bucket,key = 'annotation_1017/DJI_20210520122304_0031.bbx', im_fold = '1017_1/')\n",
    "annotation_lst = [list(x.values()) for x in annot_dict['bbox']]\n",
    "\n",
    "imre = s3client.get_object(Bucket = my_bucket, Key= '1017_2/DJI_20210520121104_0601.JPG' )\n",
    "im = Image.open(imre['Body'])\n",
    "\n",
    "# print(\"Raw image with bounding boxes:\")\n",
    "# plotting.plot_img_bbx(im, annotation_lst)\n",
    "np.array(im).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x['Key'] for x in s3client.list_objects_v2(Bucket = my_bucket, Prefix = '1017_2/')['Contents']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1srbJOyFEA9u"
   },
   "source": [
    "### Tiling \n",
    "\n",
    "In order to prepare the dataset to be used for training in our deep learning models, we must tile the large 8192 × 5460 raw drone images into smaller sizes. The size of generated images can be specified by setting parameters and is default to be 640 × 640.\n",
    "\n",
    "The following cells tiles the original dataset images and corresponding annotations in annotation files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the key/directory exist in the S3 bucket\n",
    "# a key can be the folder location or the file name\n",
    "def key_exist(my_bucket, my_key, s3 = 's3'):\n",
    "    # sujective to change for the bucket placement\n",
    "    s3_client = boto3.client(s3)\n",
    "    response = s3_client.list_objects_v2(Bucket=my_bucket, Prefix=my_key,MaxKeys=1)\n",
    "    \n",
    "    return 'Contents' in response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3client.put_object(Bucket = my_bucket, Key = 'Test123/') #this is how to make a new director in S3 bucket\n",
    "# im.save('./temp'+'/123.JPEG')\n",
    "# s3client.upload_file(Filename='./temp/123.JPEG', Bucket = my_bucket, Key = 'Test123/123.JPEG') #this is how to upload a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_csv_AWS(info_dict, output_path, empty):\n",
    "    \"\"\"\n",
    "    Function to convert (cropped images') info_dicts to annoatation csv files\n",
    "    INPUT:\n",
    "     info_dict -- output from the csv_to_dict function, containing bbox, filename, img_size\n",
    "     output_path -- folder path to store the converted csv files\n",
    "    OUTPUT:\n",
    "      an csv file(corresponding for 1 image) saved to a folder. The bndbox info in the format of (className,\n",
    "      xmin, ymin, width, height)\n",
    "    \"\"\"\n",
    "    new_bbx_buffer = []\n",
    "    schema = ['class_id', 'desc', 'x', 'y', 'width', 'height']\n",
    "    if not empty:\n",
    "        for obj in info_dict['bbox']:\n",
    "            className = obj['class']\n",
    "            desc = obj['desc']\n",
    "            xmin = obj['xmin']\n",
    "            xmax = obj['xmax']\n",
    "            ymin = obj['ymin']\n",
    "            ymax = obj['ymax']\n",
    "            # className, description, xmin, ymin, width, height\n",
    "            new_bbx_buffer.append([className, desc, int(xmin), int(ymin), int(xmax) - int(xmin), int(ymax) - int(ymin)])\n",
    "    # Name of the file to save\n",
    "    save_file_name = os.path.join('./temp', info_dict[\"file_name\"].replace('JPG', 'csv'))\n",
    "    \n",
    "    \n",
    "    # write to files\n",
    "    with open(save_file_name, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([g for g in schema])\n",
    "        if not empty:\n",
    "            writer.writerows(new_bbx_buffer)\n",
    "            \n",
    "    #Write it to the cloud\n",
    "    s3client.upload_file(Filename = save_file_name, Bucket = my_bucket, Key = output_key + info_dict[\"file_name\"].replace('JPG', 'csv'))\n",
    "\n",
    "def tile_annot(left, right, top, bottom, info_dict, i, j, crop_height, crop_width, overlap, file_dict):\n",
    "    \"\"\"\n",
    "    THIS FUNCTION calculate the new positions of bndbox in cropped img and append them to file_dict,\n",
    "    which is an info dict for that cropped img.\n",
    "\n",
    "    INPUTS:\n",
    "    left, right, top, bottom -- params for the python crop img function, coordinates for tiles.\n",
    "    origin is top left.\n",
    "    info_dict -- the info_dict we get from the csv_to_dict function.\n",
    "    overlap -- threshold for keeping a bbox.\n",
    "    \"\"\"\n",
    "    # file_dict stores info of one subimage as a dictionary. keys indicate original file name and subimage position.\n",
    "    file_dict[str(i) + '_' + str(j)] = {}\n",
    "    file_dict[str(i) + '_' + str(j)]['bbox'] = []\n",
    "    file_dict[str(i) + '_' + str(j)]['file_name'] = info_dict['file_name'][:-4] + '_' + str(i) + '_' + str(j) + '.JPG'\n",
    "    file_dict[str(i) + '_' + str(j)]['img_size'] = (right - left, bottom - top, 3)\n",
    "\n",
    "    valid = False\n",
    "    for b in info_dict['bbox']:\n",
    "        ymin = max(b['ymin'] - top, 0)\n",
    "        ymax = min(b['ymax'] - top, crop_height)\n",
    "        xmin = max(b['xmin'] - left, 0)\n",
    "        xmax = min(b['xmax'] - left, crop_width)\n",
    "        # if the bird is not in this patch, pass\n",
    "        if xmin > crop_width or xmax < 0 or ymin > crop_height or ymax < 0:\n",
    "            continue\n",
    "        else:\n",
    "            if (xmax - xmin) * (ymax - ymin) > overlap * (b['xmax'] - b['xmin']) * (b['ymax'] - b['ymin']) \\\n",
    "                    or b['xmin'] >= left and b['xmax'] <= right and b['ymin'] >= top and b['ymax'] <= bottom:\n",
    "                valid = True\n",
    "                # instance_dict is the info_dict for one patch\n",
    "                instance_dict = {}\n",
    "                # transform bbx coordinates\n",
    "                instance_dict['class'] = b['class']\n",
    "                instance_dict['desc'] = b['desc']\n",
    "                instance_dict['xmin'] = max(b['xmin'] - left, 0)\n",
    "                instance_dict['xmax'] = min(b['xmax'] - left, crop_width)\n",
    "                instance_dict['ymin'] = max(b['ymin'] - top, 0)\n",
    "                instance_dict['ymax'] = min(b['ymax'] - top, crop_height)\n",
    "\n",
    "                file_dict[str(i) + '_' + str(j)]['bbox'].append(instance_dict)\n",
    "    return valid\n",
    "\n",
    "\n",
    "# this function generates all the cropped images and all corresponding label txt files for a single file\n",
    "# file_dict stores cropped images info dict in one dictionary.\n",
    "def crop_img_AWS(s3client, my_bucket, annot_key, img_key, output_key, crop_height, crop_width, class_map = {}, overlap=0.2, annot_file_ext='csv', file_dict={}):\n",
    "    \"\"\"\n",
    "    This function crops one image and output corresponding labels.\n",
    "    Currently, this function generates the cropped images AND the corresponding csv files to output_dir\n",
    "    INPUT:\n",
    "    crop_height, crop_weight -- desired patch size.\n",
    "    overlap -- threshold for keeping bbx.\n",
    "    annot_file_ext -- annotation file extension\n",
    "    \"\"\"\n",
    "\n",
    "    info_dict = csv_to_dict_AWS(bucket_name= my_bucket,key = annot_key, im_fold = img_key)\n",
    "    \n",
    "    img_height, img_width, img_depth = info_dict['img_size']\n",
    "    \n",
    "    image = s3client.get_object(Bucket = my_bucket, Key= img_key + annot_key.split('/')[-1].replace(annot_file_ext, 'JPG') )\n",
    "    \n",
    "    im = Image.open(image['Body'], 'r')\n",
    "    \n",
    "    file_name = annot_key.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    # go through the image from top left corner\n",
    "    for i in range(img_height // crop_height + 1):\n",
    "\n",
    "        for j in range(img_width // crop_width + 1):\n",
    "\n",
    "            if j < (img_width // crop_width) and i < (img_height // crop_height):\n",
    "                left = j * crop_width\n",
    "                right = (j + 1) * crop_width\n",
    "                top = i * crop_height\n",
    "                bottom = (i + 1) * crop_height\n",
    "\n",
    "            elif j == img_width // crop_width and i < (img_height // crop_height):\n",
    "                left = img_width - crop_width\n",
    "                right = img_width\n",
    "                top = i * crop_height\n",
    "                bottom = (i + 1) * crop_height\n",
    "\n",
    "            # if rectangles left on edges, take subimage of crop_height*crop_width by taking a part from within.\n",
    "            elif i == img_height // crop_height and j < (img_width // crop_width):\n",
    "                left = j * crop_width\n",
    "                right = (j + 1) * crop_width\n",
    "                top = img_height - crop_height\n",
    "                bottom = img_height\n",
    "\n",
    "            else:\n",
    "                left = img_width - crop_width\n",
    "                right = img_width\n",
    "                top = img_height - crop_height\n",
    "                bottom = img_height\n",
    "                \n",
    "            # even if no birds in cropped img, keep the cropped image\n",
    "            \n",
    "            if tile_annot(left, right, top, bottom, info_dict, i, j, crop_height, crop_width, overlap, file_dict):\n",
    "                # print('Generating segmentation at position: ', left, top, right, bottom)\n",
    "                \n",
    "                c_img = im.crop((left, top, right, bottom))\n",
    "                c_img_name = file_name + '_' + str(i) + '_' + str(j) +'.JPEG'\n",
    "                \n",
    "                # write all this is the temporary folder in the current working directory\n",
    "                c_img.save('./temp'+'/'+c_img_name)\n",
    "                \n",
    "                #uploading it to the bucket storage\n",
    "                s3client.upload_file(Filename = './temp'+'/'+c_img_name, Bucket = my_bucket, Key = output_key +c_img_name)\n",
    "                \n",
    "                \n",
    "\n",
    "    # output the file_dict to a folder of csv files containing labels for each cropped file\n",
    "    for b in file_dict:\n",
    "        if file_dict[b]['bbox'] == []:\n",
    "            empty = True\n",
    "            continue\n",
    "        else:\n",
    "            empty = False\n",
    "            dict_to_csv_AWS(file_dict[b], empty=empty, output_path=output_key)\n",
    "\n",
    "    return file_dict\n",
    "\n",
    "\n",
    "def crop_dataset_AWS(bucket, data_key, output_key, annot_key, annot_file_ext = 'csv', class_map = {}, crop_height=640, crop_width=640):\n",
    "    \"\"\"\n",
    "    :param data_dir: image set directory\n",
    "    :param output_dir: output directory\n",
    "    :param annot_file_ext: annotation file extension\n",
    "    :param crop_height: image height after tiling, default 640\n",
    "    :param crop_width: image width after tiling, default 640\n",
    "    \"\"\"\n",
    "    s3client = boto3.client('s3') # start grabbing or makign directory in S3 bucket\n",
    "    \n",
    "    \n",
    "    if not key_exist(my_bucket = bucket, my_key = output_key): # this function works only for S3 bucket, will change if we need to modularize this\n",
    "        print(f\"Creating output directory at in S3 bucket called: {output_key}\")\n",
    "        s3client.put_object(Bucket = bucket, Key = output_key)\n",
    "        \n",
    "    # making temporary folder in the current directory\n",
    "    if not os.path.exists('./temp'):\n",
    "        print(f\"Creating temp folder\")\n",
    "        os.makedirs('./temp')\n",
    "\n",
    "                            \n",
    "\n",
    "    # find all the files inside the annotated folders                            \n",
    "    if annot_file_ext == 'csv':\n",
    "        files = [x['Key'] for x in s3client.list_objects_v2(Bucket = my_bucket, Prefix = annot_key)['Contents']]\n",
    "    if annot_file_ext == 'bbx':\n",
    "        files = [x['Key'] for x in s3client.list_objects_v2(Bucket = my_bucket, Prefix = annot_key)['Contents']]\n",
    "                            \n",
    "                            \n",
    "    # for each annotated file, crop the image and place it into the output directory\n",
    "    for f in tqdm(files, desc='Cropping files'):\n",
    "        crop_img_AWS(s3client = s3client, my_bucket = bucket, annot_key=f ,img_key = data_key, output_key = output_key, crop_height=crop_height, crop_width=crop_width, class_map=class_map,\n",
    "                 annot_file_ext=annot_file_ext)\n",
    "    \n",
    "    shutil.rmtree('./temp')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GfSuT1ioQFOX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temp folder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d55cbc85e8444198227c202524b2bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cropping files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from Audubon_F21.utils.cropping import crop_dataset\n",
    "\n",
    "# # data_dir is the path that contains both images and annotations (image: jpg; annotation: csv or bbx)\n",
    "# data_dir = './data/raw' # data directory folder \n",
    "# # output dir is the path where you want to output new files. Please use the folder you defined above.\n",
    "# output_dir = './data/tiled'\n",
    "\n",
    "# from utils.cropping import csv_to_dict_AWS\n",
    "\n",
    "data_key = '1017_1/'\n",
    "output_key = 'test_crop/'\n",
    "annot_key = 'annotation_1017/'\n",
    "\n",
    "crop_dataset_AWS(bucket = my_bucket, data_key = data_key, output_key=output_key\n",
    "                 , annot_key = annot_key, annot_file_ext = 'bbx',crop_height=640, crop_width=640)\n",
    "\n",
    "# crop_dataset(data_dir, output_dir, annot_file_ext = 'bbx', crop_height = 640, crop_width = 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split_AWS(s3client, my_bucket, file_key, train_key, val_key, test_key, train_frac=0.8, val_frac=0.1):\n",
    "    \"\"\"\n",
    "    :param file_dir: crop_dataset()'s output path:\n",
    "    :param output_dir: an empty folder\n",
    "    :param train_frac: fraction for training\n",
    "    :param val_frac: fraction for validation, 1-train-val will be fraction for test\n",
    "    \"\"\"\n",
    "            \n",
    "    # get all the files from s3 bucket (because its larger than 1000 we beed to use paginator)\n",
    "    total_file = s3client.get_paginator('list_objects_v2').paginate(Bucket=my_bucket, Prefix=file_key)\n",
    "    \n",
    "    img_list = [f['Key'] for p in total_file for f in p['Contents'] if f['Key'].split('.')[-1] == 'JPEG' ]\n",
    "    img_list = img_list[1:]\n",
    "    \n",
    "    random.Random(4).shuffle(img_list)\n",
    "    \n",
    "    csv_list = [f.replace('JPEG', 'csv') for f in img_list]\n",
    "    number_img = len(img_list)\n",
    "    train_sz = int(number_img * train_frac)\n",
    "    val_sz = int(number_img * val_frac)\n",
    "    \n",
    "   \n",
    "    s3 = boto3.resource('s3')\n",
    "\n",
    "    \n",
    "    for i in range(number_img):\n",
    "        \n",
    "        copy_source_img = {'Bucket': my_bucket, 'Key': img_list[i]}\n",
    "        copy_source_csv = {'Bucket': my_bucket, 'Key': csv_list[i]}\n",
    "        if i < train_sz:\n",
    "#             print('this is im ', train_key+img_list[i].split('/')[-1])\n",
    "#             print('this is csv ', train_key+csv_list[i].split('/')[-1])\n",
    "            s3.meta.client.copy(copy_source_img, my_bucket, train_key+img_list[i].split('/')[-1])\n",
    "            s3.meta.client.copy(copy_source_csv, my_bucket, train_key+csv_list[i].split('/')[-1])\n",
    "            \n",
    "        elif i < train_sz+val_sz:\n",
    "#             print('this is im ', val_key+img_list[i].split('/')[-1])\n",
    "#             print('this is csv ', val_key+csv_list[i].split('/')[-1])\n",
    "            s3.meta.client.copy(copy_source_img, my_bucket, val_key+img_list[i].split('/')[-1])\n",
    "            s3.meta.client.copy(copy_source_csv, my_bucket, val_key+csv_list[i].split('/')[-1])\n",
    "            \n",
    "        else:\n",
    "#             print('this is im ', test_key+img_list[i].split('/')[-1])\n",
    "#             print('this is csv ', test_key+csv_list[i].split('/')[-1])\n",
    "            s3.meta.client.copy(copy_source_img, my_bucket, test_key+img_list[i].split('/')[-1])\n",
    "            s3.meta.client.copy(copy_source_csv, my_bucket, test_key+csv_list[i].split('/')[-1])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_to_folder(s3client, my_bucket, data_dirs):\n",
    "    ''' \n",
    "    Get all the data from each of the train test val dirs and download them into local\n",
    "    '''\n",
    "    \n",
    "    dd = [x['Prefix'] for x in s3client.list_objects_v2(Bucket=my_bucket, Prefix=data_dirs,Delimiter='/')['CommonPrefixes']]\n",
    "\n",
    "    for data_key in dd:\n",
    "        total_file = s3client.get_paginator('list_objects_v2').paginate(Bucket=my_bucket, Prefix=data_key)\n",
    "        \n",
    "        files = [f['Key'] for p in total_file for f in p['Contents']]\n",
    "        files = files[1:]\n",
    "        \n",
    "        for file in files:\n",
    "            if file.split('.')[-1] == 'JPEG':\n",
    "#                 print('saving image: ', file)\n",
    "                im = s3client.get_object(Bucket = my_bucket, Key = file)\n",
    "                image = Image.open(im['Body'])\n",
    "                image.save('./'+file)\n",
    "            else:\n",
    "#                 print('saving pd: ', file)\n",
    "                annt = s3client.get_object(Bucket = my_bucket, Key = file)\n",
    "                annt = pd.read_csv(annt['Body'])\n",
    "                annt.to_csv('./' + file)\n",
    "                \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving image:  data/train/DJI_20210520122418_0076_1_4.JPEG\n",
      "saving pd:  data/train/DJI_20210520122418_0076_1_4.csv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-H0_4KdPEEJZ"
   },
   "source": [
    "### Split dataset into training, validation, and test \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qfb8Q6BtQQ5F"
   },
   "outputs": [],
   "source": [
    "# len([f['Key'] for f in s3client.list_objects_v2(Bucket=my_bucket, Prefix=output_key, MaxKeys = 50000)['Contents']])\n",
    "\n",
    "\n",
    "# paginator = s3client.get_paginator('list_objects_v2')paginate(Bucket=my_bucket, Prefix=output_key)\n",
    "# pages = paginator.\n",
    "\n",
    "# len([f['Key'] for p in pages for f in p['Contents'] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./data/train\n",
    "!mkdir -p ./data/val\n",
    "!mkdir -p ./data/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'PX8C0G1ZB5GQY4E0',\n",
       "  'HostId': 'e+XR1YKQJcDy/cr8nXiOJffUU4iiHJXz6cJCNRBzliaOOKVXK7wcYMVy3V/tV+ZgqDS6XWaTkbw=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'e+XR1YKQJcDy/cr8nXiOJffUU4iiHJXz6cJCNRBzliaOOKVXK7wcYMVy3V/tV+ZgqDS6XWaTkbw=',\n",
       "   'x-amz-request-id': 'PX8C0G1ZB5GQY4E0',\n",
       "   'date': 'Fri, 11 Feb 2022 04:39:47 GMT',\n",
       "   'etag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"d41d8cd98f00b204e9800998ecf8427e\"'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3client = boto3.client('s3')\n",
    "\n",
    "train_dir = 'data/train/'\n",
    "val_dir = 'data/val/'\n",
    "test_dir = 'data/test/'\n",
    "\n",
    "\n",
    "s3client.put_object(Bucket = my_bucket, Key = train_dir)\n",
    "s3client.put_object(Bucket = my_bucket, Key = val_dir)\n",
    "s3client.put_object(Bucket = my_bucket, Key = test_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_test_split_AWS(s3client, my_bucket, file_key = 'test_crop/', train_key = train_dir,\n",
    "                         val_key = val_dir, test_key = test_dir, train_frac=0.8, val_frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket_to_folder(s3client, my_bucket, data_dirs = 'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_file = s3client.get_paginator('list_objects_v2').paginate(Bucket=my_bucket, Prefix=train_dir)\n",
    "    \n",
    "img_list = [f['Key'] for p in total_file for f in p['Contents'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gEL7rjsEsSk"
   },
   "outputs": [],
   "source": [
    "# rewrite this code later\n",
    "\n",
    "# # Distribution of bird species for train, val, and test sets\n",
    "# # data directory folders \n",
    "# data_dir = 'data/split'\n",
    "# dirs = [d for d in os.listdir(data_dir)]\n",
    "\n",
    "# # Load CSV files \n",
    "# for d in dirs: \n",
    "#   target_data = []\n",
    "#   for f in glob.glob(os.path.join(data_dir,d,'*.csv')): \n",
    "#     target_data.append(pd.read_csv(f, header=0, \n",
    "#                               names = [\"class_id\", \"class_name\", \"x\", \"y\", \"width\", \"height\"]) )\n",
    "#   target_data = pd.concat(target_data, axis=0, ignore_index=True)\n",
    "\n",
    "#   # Visualize dataset \n",
    "#   print(f'\\n {d} - Bird Species Distribution')\n",
    "#   print(target_data[\"class_name\"].value_counts())\n",
    "#   print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_moYO6MEsyq"
   },
   "source": [
    "## Modeling \n",
    "\n",
    "The primary models used to detect birds within the drone images are convolutional neural network (CNN) based object detectors. To implement these models, we utilize [Detectron2](https://github.com/facebookresearch/detectron2), Facebook AI Research's next generation library that provides state-of-the-art detection and segmentation algorithms. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yU-y99FBF7nm"
   },
   "source": [
    "### Setup dataloaders \n",
    "\n",
    "The following cell registers the training, validation, and testing datasets with Detectron2's dataset catalogs. Note that we register both a version that utilizes both a singular \"bird-only\" label and the bird species labels. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5ruFu59cxbp"
   },
   "outputs": [],
   "source": [
    "# from Audubon_F21.utils.dataloader import register_datasets\n",
    "\n",
    "# data_dir = './data/split'\n",
    "# img_ext='.JPEG'\n",
    "# dirs = [os.path.join(data_dir,d) for d in os.listdir(data_dir)]\n",
    "\n",
    "# # Bird species used by object detector. Species contained in dataset that are \n",
    "# # not contained in this list will be categorized as an \"Unknown Bird\"\n",
    "# BIRD_SPECIES = [\"Brown Pelican\", \"Laughing Gull\", \"Mixed Tern\",\n",
    "#                 \"Great Blue Heron\",\"Great Egret/White Morph\"]\n",
    "\n",
    "# # Bounding box colors for bird species (used when plotting images)\n",
    "# BIRD_SPECIES_COLORS = [(255,0,0), (255,153,51), (0, 255, 0), \n",
    "#                        (0,0,255), (255, 51, 255)]\n",
    "\n",
    "# register_datasets(dirs,img_ext,BIRD_SPECIES,bird_species_colors=BIRD_SPECIES_COLORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "from cv2 import imread\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "def get_bird_only_dicts_AWS(s3client, my_bucket, data_key, img_ext='.JPG'):\n",
    "    \"\"\"\n",
    "    Format dataset to detectron2 standard format. \n",
    "    INPUTS: \n",
    "    data_dir -- directory containing dataset files \n",
    "    img_ext -- file extension for images in dataset\n",
    "    OUTPUTS: \n",
    "    dataset_dicts -- list of dictionaries in detectron2 standard format\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_dicts = []\n",
    "    \n",
    "    # get all the filenames for the annotated files on the S3 bucket \n",
    "    \n",
    "    total_file = s3client.get_paginator('list_objects_v2').paginate(Bucket=my_bucket, Prefix=data_key)\n",
    "    \n",
    "    files = [f['Key'] for p in total_file for f in p['Contents'] if f['Key'].split('.')[-1] == 'csv']\n",
    "    \n",
    "#     files = [x['Key'] for x in s3client.list_objects_v2(Bucket=my_bucket, Prefix=data_key) ['Contents'] if x['Key'].split('.')[-1] == 'csv'] \n",
    "    \n",
    "    \n",
    "    print(files)\n",
    "    for idx,file_csv in enumerate(files): \n",
    "        record = {}\n",
    "        \n",
    "        print(file_csv.replace('csv', 'JPEG'))\n",
    "        imre = s3client.get_object(Bucket = my_bucket, Key = file_csv.replace('csv', 'JPEG'))\n",
    "        \n",
    "        im = Image.open(imre['Body'])\n",
    "        height, width, rgb = np.array(im).shape\n",
    "        record[\"file_name\"] = file_csv.replace('csv', 'JPEG')\n",
    "        record[\"image_id\"] = idx\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "\n",
    "        # annotations\n",
    "\n",
    "        annt = s3client.get_object(Bucket = my_bucket, Key = file_csv)\n",
    "\n",
    "        imgs_anns_df = pd.read_csv(annt['Body'], header=0, names = [\"class_id\", \"class_name\", \"x\", \"y\", \"width\", \"height\"])\n",
    "        # skip empty images\n",
    "\n",
    "        if imgs_anns_df.shape[0]==0: \n",
    "            continue\n",
    "        # remove annotations for trash \n",
    "        imgs_anns_df = imgs_anns_df[imgs_anns_df[\"class_name\"]!=\"Trash/Debris\"]\n",
    "\n",
    "        objs = []\n",
    "        for idx, row in imgs_anns_df.iterrows():  \n",
    "            obj = {\n",
    "              \"bbox\": [row[\"x\"], row[\"y\"], row[\"width\"], row[\"height\"]],      \n",
    "              \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "              \"category_id\": 0,\n",
    "            }\n",
    "            objs.append(obj)\n",
    "\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "        \n",
    "    return dataset_dicts\n",
    "\n",
    "\n",
    "def get_bird_species_dicts_AWS(s3client, my_bucket, data_key,class_names,img_ext='.JPG',unknown_bird_category=True,skip_empty_imgs=True):\n",
    "    \"\"\"\n",
    "    Format dataset to detectron2 standard format. \n",
    "    INPUTS: \n",
    "    data_dir -- directory containing dataset files \n",
    "    img_ext -- file extension for images in dataset\n",
    "    class_names -- names of bird species\n",
    "    skip_empty_imgs -- keep images with no birds \n",
    "    OUTPUTS: \n",
    "    dataset_dicts -- list of dictionaries in detectron2 standard format\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_dicts = []\n",
    "    \n",
    "    # get all the filenames for the annotated files on the S3 bucket\n",
    "    total_file = s3client.get_paginator('list_objects_v2').paginate(Bucket=my_bucket, Prefix=data_key)\n",
    "    \n",
    "    files = [f['Key'] for p in total_file for f in p['Contents'] if f['Key'].split('.')[-1] == 'csv']\n",
    "    \n",
    "#     files = [x['Key'] for x in s3client.list_objects_v2(Bucket=my_bucket, Prefix=data_key) ['Contents'] if x['Key'].split('.')[-1] == 'csv']\n",
    "    \n",
    "    for idx,file_csv in enumerate(files): \n",
    "        record = {}\n",
    "\n",
    "        # image attributes\n",
    "        imre = s3client.get_object(Bucket = my_bucket, Key = file_csv.replace('csv', 'JPEG'))\n",
    "        im = Image.open(imre['Body'])\n",
    "        height, width, rgb = np.array(im).shape\n",
    "        record[\"file_name\"] = file_csv.replace('csv', 'JPEG')\n",
    "        record[\"image_id\"] = idx\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "\n",
    "        # annotations\n",
    "\n",
    "        annt = s3client.get_object(Bucket = my_bucket, Key = file_csv)\n",
    "\n",
    "        imgs_anns_df = pd.read_csv(annt['Body'], header=0, names = [\"class_id\", \"class_name\", \"x\", \"y\", \"width\", \"height\"])\n",
    "\n",
    "        # skip empty images\n",
    "        if skip_empty_imgs and imgs_anns_df.shape[0]==0:\n",
    "            continue\n",
    "        # remove annotations for trash\n",
    "        imgs_anns_df = imgs_anns_df[imgs_anns_df[\"class_name\"]!=\"Trash/Debris\"]\n",
    "\n",
    "        objs = []\n",
    "        for idx, row in imgs_anns_df.iterrows():\n",
    "            obj = None\n",
    "            for id, class_name in enumerate(class_names):\n",
    "                if class_name in row[\"class_name\"]:\n",
    "                    obj = {\"bbox\": [row[\"x\"], row[\"y\"], row[\"width\"], row[\"height\"]],\n",
    "                           \"bbox_mode\": BoxMode.XYWH_ABS, \"category_id\": id,}\n",
    "                    objs.append(obj)\n",
    "                    break\n",
    "            if obj is None:\n",
    "                if unknown_bird_category:\n",
    "                    obj = {\"bbox\": [row[\"x\"], row[\"y\"], row[\"width\"], row[\"height\"]],\n",
    "                           \"bbox_mode\": BoxMode.XYWH_ABS, \"category_id\": len(class_names)}\n",
    "                    objs.append(obj)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "        \n",
    "        return dataset_dicts\n",
    "\n",
    "\n",
    "def register_datasets_AWS(s3client, my_bucket, data_dirs, img_ext, birds_species_names, bird_species_colors=None):\n",
    "    \"\"\"\n",
    "    Register dataset as part of Detectron2's dataset and metadataset catalogs\n",
    "    For each dataset directory to be registered, a \"bird-only\" and \"bird-species\" dataset will be registered\n",
    "    INPUTS:\n",
    "    data_dirs: list of directories containing dataset images to be registered\n",
    "    img_ext: file extension for images in dataset\n",
    "    birds_species_names: names of bird species to be registered. Species not in this list will be registered as\n",
    "                        \"Unknown Bird\"\n",
    "    bird_species_colors: List of colors for corresponding to bird species to be used for visualizations. Color\n",
    "                         format should be tuple containing RGB values between 0-255 eg. (255,0,0) for red\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the list of directors for the train and split \n",
    "    dd = [x['Prefix'] for x in s3client.list_objects_v2(Bucket=my_bucket, Prefix=data_dirs,Delimiter='/')['CommonPrefixes']]\n",
    "\n",
    "    for data_key in dd:\n",
    "        d = data_key.split('/')[1]\n",
    "        # birds only section #########################################################################################\n",
    "        \n",
    "        # check if we have the bird only in the dataset catalog\n",
    "        if f\"birds_only_{d}\" in DatasetCatalog.list():\n",
    "            DatasetCatalog.remove(f\"birds_only_{d}\")\n",
    "        # this function will register the data set into detectron2 package\n",
    "        DatasetCatalog.register(f\"birds_only_{d}\",lambda d=d:get_bird_only_dicts_AWS(s3client, my_bucket, data_key, img_ext))\n",
    "        \n",
    "        if f\"birds_only_{d}\" in MetadataCatalog.list():\n",
    "            MetadataCatalog.remove(f\"birds_only_{d}\")\n",
    "        MetadataCatalog.get(f\"birds_only_{d}\").set(thing_classes=[\"Bird\"])\n",
    "\n",
    "        # birds species section #########################################################################################\n",
    "        if f\"birds_species_{d}\" in DatasetCatalog.list():\n",
    "            DatasetCatalog.remove(f\"birds_species_{d}\")\n",
    "        DatasetCatalog.register(f\"birds_species_{d}\", lambda d=d:get_bird_species_dicts_AWS(s3client\n",
    "                                                                                            ,my_bucket, data_key,birds_species_names,img_ext='.JEPG',unknown_bird_category=True))\n",
    "        if f\"birds_species_{d}\" in MetadataCatalog.list():\n",
    "            MetadataCatalog.remove(f\"birds_species_{d}\")\n",
    "        MetadataCatalog.get(f\"birds_species_{d}\").set(thing_classes=birds_species_names + [\"Unknown Bird\"])\n",
    "        \n",
    "        if bird_species_colors is not None:\n",
    "            MetadataCatalog.get(f\"birds_species_{d}\").set(thing_colors=bird_species_colors + [(0, 0, 0)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 12\n",
    "f = lambda x=x: print(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "im = s3client.get_object(Bucket = my_bucket, Key = 'data/val/DJI_20210520122307_0033_3_10.csv')\n",
    "# Image.open(im['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dd = [x['Key'] for x in s3client.list_objects_v2(Bucket=my_bucket, Prefix=data_dirs,Delimiter='/')['Contents']] \n",
    "\n",
    "[x['Prefix'] for x in s3client.list_objects_v2(Bucket=my_bucket, Prefix=data_dirs,Delimiter='/')['CommonPrefixes']]\n",
    "# for i in dd:\n",
    "# #     if i[-1] == '/':\n",
    "# #         print(i)\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from utils.dataloader_aws import register_datasets_aws\n",
    "\n",
    "# BIRD_SPECIES = [\"Brown Pelican\", \"Laughing Gull\", \"Mixed Tern\",\n",
    "#                 \"Great Blue Heron\",\"Great Egret/White Morph\"]\n",
    "\n",
    "# # Bounding box colors for bird species (used when plotting images)\n",
    "# BIRD_SPECIES_COLORS = [(255,0,0), (255,153,51), (0, 255, 0), \n",
    "#                        (0,0,255), (255, 51, 255)]\n",
    "\n",
    "# data_dirs = 'data/'\n",
    "\n",
    "\n",
    "# register_datasets_AWS(s3client,my_bucket, data_dirs, img_ext = 'JPEG',birds_species_names =  BIRD_SPECIES, bird_species_colors=BIRD_SPECIES_COLORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataloader import register_datasets\n",
    "\n",
    "data_dir = './data'\n",
    "img_ext='.JPEG'\n",
    "dirs = [os.path.join(data_dir,d) for d in os.listdir(data_dir)]\n",
    "\n",
    "# Bird species used by object detector. Species contained in dataset that are \n",
    "# not contained in this list will be categorized as an \"Unknown Bird\"\n",
    "BIRD_SPECIES = [\"Brown Pelican\", \"Laughing Gull\", \"Mixed Tern\",\n",
    "                \"Great Blue Heron\",\"Great Egret/White Morph\"]\n",
    "\n",
    "# Bounding box colors for bird species (used when plotting images)\n",
    "BIRD_SPECIES_COLORS = [(255,0,0), (255,153,51), (0, 255, 0), \n",
    "                       (0,0,255), (255, 51, 255)]\n",
    "\n",
    "register_datasets(dirs,img_ext,BIRD_SPECIES,bird_species_colors=BIRD_SPECIES_COLORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x['Key'] for x in s3client.list_objects_v2(Bucket=my_bucket, Prefix='data/val/')['Contents']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVGJPdoKF-mj"
   },
   "source": [
    "### Training \n",
    "\n",
    "The following cells train a RetinaNet and Faster R-CNN model with a ResNet-50 FPN backbone. The model weights are initialized from a model pretrained on the MS COCO dataset. The training loop is based on Detectron2's Default Trainer.  Hyperparameters can be tweaked! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cq7HuVsoKQET"
   },
   "source": [
    "#### Bird-only model\n",
    "\n",
    "The bird-only model simplies localizes all birds and does not distiguish bird species. We utilize RetinaNet for faster performance rather than accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7pAw0SSKkh5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading config /opt/conda/lib/python3.6/site-packages/detectron2/model_zoo/configs/COCO-Detection/../Base-RetinaNet.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/11 05:02:15 d2.engine.defaults]: \u001b[0mModel:\n",
      "RetinaNet(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelP6P7(\n",
      "      (p6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): RetinaNetHead(\n",
      "    (cls_subnet): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU()\n",
      "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): ReLU()\n",
      "      (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU()\n",
      "    )\n",
      "    (bbox_subnet): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU()\n",
      "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): ReLU()\n",
      "      (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU()\n",
      "    )\n",
      "    (cls_score): Conv2d(256, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bbox_pred): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (anchor_generator): DefaultAnchorGenerator(\n",
      "    (cell_anchors): BufferList()\n",
      "  )\n",
      ")\n",
      "\u001b[32m[02/11 05:02:17 d2.data.build]: \u001b[0mRemoved 2 images with no usable annotations. 61 images left.\n",
      "\u001b[32m[02/11 05:02:17 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[02/11 05:02:17 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[02/11 05:02:17 d2.data.common]: \u001b[0mSerializing 61 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[02/11 05:02:17 d2.data.common]: \u001b[0mSerialized dataset takes 0.01 MiB\n",
      "\u001b[32m[02/11 05:02:17 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[02/11 05:02:18 d2.data.common]: \u001b[0mSerializing 63 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[02/11 05:02:18 d2.data.common]: \u001b[0mSerialized dataset takes 0.02 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'head.cls_score.weight' to the model due to incompatible shapes: (720, 256, 3, 3) in the checkpoint but (9, 256, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'head.cls_score.bias' to the model due to incompatible shapes: (720,) in the checkpoint but (9,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mhead.cls_score.{bias, weight}\u001b[0m\n",
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mpixel_mean\u001b[0m\n",
      "  \u001b[35mpixel_std\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/11 05:02:19 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    }
   ],
   "source": [
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from utils.trainer import Trainer\n",
    "\n",
    "# setup training logger \n",
    "setup_logger()\n",
    "\n",
    "model_name = \"retinanet_R_50_FPN_1x\"\n",
    "\n",
    "# Create detectron2 config \n",
    "cfg = get_cfg()\n",
    "cfg.MODEL.DEVICE='cpu'\n",
    "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
    "cfg.merge_from_file(model_zoo.get_config_file(f\"COCO-Detection/{model_name}.yaml\"))\n",
    "# Get pretrained model from MS COCO\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(f\"COCO-Detection/{model_name}.yaml\")\n",
    "\n",
    "# add datasets used for training and validation \n",
    "cfg.DATASETS.TRAIN = (\"birds_only_train\",)\n",
    "cfg.DATASETS.TEST = (\"birds_only_val\",)\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 1\n",
    "cfg.SOLVER.IMS_PER_BATCH = 8\n",
    "cfg.SOLVER.BASE_LR = 1e-3 # pick a good LR\n",
    "cfg.SOLVER.GAMMA = 0.1 # pick a good LR\n",
    "cfg.SOLVER.WARMUP_ITERS = 1\n",
    "cfg.MODEL.RETINANET.NUM_CLASSES = 1\n",
    "cfg.SOLVER.MAX_ITER = 1000\n",
    "cfg.SOLVER.STEPS = [500,]\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 500\n",
    "\n",
    "cfg.OUTPUT_DIR = f\"./output/multibirds_{model_name}\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# train on bird species\n",
    "trainer = Trainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfDQZrLuKkn_"
   },
   "source": [
    "#### Bird species \n",
    "\n",
    "The bird species model both localizes and classifies bird species. We registered the species to be classifed in the above dataloader (see BIRD_SPECIES list). We utilize Faster R-CNN for better performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gmB4XbGUdI0u"
   },
   "outputs": [],
   "source": [
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from Audubon_F21.utils.trainer import Trainer\n",
    "\n",
    "# setup training logger \n",
    "setup_logger()\n",
    "\n",
    "model_name = \"faster_rcnn_R_50_FPN_1x\"\n",
    "\n",
    "# Create detectron2 config \n",
    "cfg = get_cfg()\n",
    "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
    "cfg.merge_from_file(model_zoo.get_config_file(f\"COCO-Detection/{model_name}.yaml\"))\n",
    "# Get pretrained model from MS COCO\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(f\"COCO-Detection/{model_name}.yaml\")\n",
    "\n",
    "# add datasets used for training and validation \n",
    "cfg.DATASETS.TRAIN = (\"birds_species_train\",)\n",
    "cfg.DATASETS.TEST = (\"birds_species_val\",)\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.SOLVER.IMS_PER_BATCH = 8\n",
    "cfg.SOLVER.BASE_LR = 1e-3 # pick a good LR\n",
    "cfg.SOLVER.GAMMA = 0.1 # pick a good LR\n",
    "cfg.SOLVER.WARMUP_ITERS = 1\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(BIRD_SPECIES)\n",
    "cfg.SOLVER.MAX_ITER = 1000\n",
    "cfg.SOLVER.STEPS = [500,]\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 500\n",
    "\n",
    "cfg.OUTPUT_DIR = f\"./output/multibirds_{model_name}\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# train on bird species\n",
    "trainer = Trainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8liZIl4GCvV"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "The following cell outputs various evaluation metrics, plots, and images. Please read more about the [COCO evaluation metrics](https://cocodataset.org/#detection-eval) to understand how the AP metrics are calculated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hE1OYaOPKf1s"
   },
   "source": [
    "#### Bird-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oi41HGjtKf-_"
   },
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultPredictor\n",
    "from Audubon_F21.utils.evaluation import plot_precision_recall\n",
    "\n",
    "cfg.MODEL.WEIGHTS = \"./output/bird_only_retinanet_R_50_FPN_1x/model_final.pth\" # path to the model we just trained\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "print('validation inference:')\n",
    "val_precisions, val_max_recalls = get_precisions_recalls(cfg, predictor, \"birds_only_val\")\n",
    "plot_precision_recall(val_precisions, val_max_recalls, [\"Bird\"])\n",
    "\n",
    "print('test inference:')\n",
    "test_precisions, test_max_recalls = get_precisions_recalls(cfg, predictor, \"birds_only_test\")\n",
    "plot_precision_recall(test_precisions, test_max_recalls, [\"Bird\"])\n",
    "\n",
    "# Plot examples of detections on validation and testing tiled images \n",
    "for d in [\"val\", \"test\"]:\n",
    "    dataset_dicts = DatasetCatalog.get(f\"birds_only_{d}\")\n",
    "    print(f'\\n {d} examples:')\n",
    "    for k in random.sample(dataset_dicts, 2):\n",
    "        im = cv2.imread(k[\"file_name\"])\n",
    "        outputs = predictor(im)\n",
    "        outputs = outputs[\"instances\"].to(\"cpu\")\n",
    "        outputs = outputs[outputs.scores > 0.5]\n",
    "        v = Visualizer(im[:, :, ::-1],\n",
    "                        metadata=MetadataCatalog.get(f\"birds_only_{d}\"),\n",
    "                        scale=0.5)\n",
    "        out = v.draw_instance_predictions(outputs)\n",
    "        cv2.imshow(f'{d} prediction {i}',out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmrAC_sTKd77"
   },
   "source": [
    "#### Bird species "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vkypxNYPqUW3"
   },
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultPredictor\n",
    "from Audubon_F21.utils.evaluation import plot_precision_recall\n",
    "\n",
    "cfg.MODEL.WEIGHTS = \"./output/multibirds_faster_rcnn_R_50_FPN_1x/model_final.pth\" # path to the model we just trained\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "print('validation inference:')\n",
    "val_precisions, val_max_recalls = get_precisions_recalls(cfg, predictor, \"birds_species_val\")\n",
    "plot_precision_recall(val_precisions, val_max_recalls, BIRD_SPECIES + [\"Unknown Bird\"],\n",
    "                      BIRD_SPECIES_COLORS + [(0, 0, 0)])\n",
    "\n",
    "print('test inference:')\n",
    "test_precisions, test_max_recalls = get_precisions_recalls(cfg, predictor, \"birds_species_test\")\n",
    "plot_precision_recall(test_precisions, test_max_recalls, BIRD_SPECIES + [\"Unknown Bird\"],\n",
    "                      BIRD_SPECIES_COLORS + [(0, 0, 0)])\n",
    "\n",
    "# Plot examples of detections on validation and testing tiled images \n",
    "for d in [\"val\", \"test\"]:\n",
    "    dataset_dicts = DatasetCatalog.get(f\"birds_species_{d}\")\n",
    "    print(f'\\n {d} examples:')\n",
    "    for k in random.sample(dataset_dicts, 2):\n",
    "        im = cv2.imread(k[\"file_name\"])\n",
    "        outputs = predictor(im)\n",
    "        outputs = outputs[\"instances\"].to(\"cpu\")\n",
    "        outputs = outputs[outputs.scores > 0.5]\n",
    "        v = Visualizer(im[:, :, ::-1],\n",
    "                        metadata=MetadataCatalog.get(f\"birds_species_{d}\"),\n",
    "                        scale=0.5,\n",
    "                        instance_mode=ColorMode.SEGMENTATION)\n",
    "        out = v.draw_instance_predictions(outputs)\n",
    "        cv2.imshow(f'{d} prediction {i}',out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjFXDR2GFvG6"
   },
   "source": [
    "## Running trained model on dataset\n",
    "\n",
    "The following cells run a pretrained model on a dataset containing only raw images. It generates an output csv file containing the predicted bounding boxes after non-maximal suppression. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVOe51dbBIgs"
   },
   "source": [
    "### Tiling\n",
    "\n",
    "The tiling step in the detection pipeline is done using a sliding window. The sub-images are deliberately generated to have a significant proportion of overlapping with adjacent sub-images. The level of overlapping can be specified by setting a parameter. The reason why we want to have the overlapping is because we can ensure that there is at least one complete version of each bird in one of the sub-images. We then try to eliminate overlapping predicted bounding boxes for the same bird by using non-maximum suppression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMbdS6L3Elbr"
   },
   "outputs": [],
   "source": [
    "from Audubon_F21.utils.cropping import crop_dataset_img_only\n",
    "\n",
    "# create folder to contain tiled images\n",
    "!rm -rf './data/crop'\n",
    "!mkdir -p './data/crop'\n",
    "\n",
    "# perform tiling on images \n",
    "data_dir = './data/raw' # data directory folder \n",
    "output_dir = './data/crop'\n",
    "img_ext = '.JPG'\n",
    "CROP_WIDTH = 640 \n",
    "CROP_HEIGHT = 640\n",
    "SLIDING_SIZE = 400 \n",
    "crop_dataset_img_only(data_dir, img_ext, output_dir, crop_height=CROP_HEIGHT, crop_width=CROP_WIDTH, sliding_size=SLIDING_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoowvdjRBK1w"
   },
   "source": [
    "### Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K62BnBAi9_EN"
   },
   "outputs": [],
   "source": [
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from Audubon_F21.utils.evaluation import evaluate_full_pipeline\n",
    "\n",
    "# create list of tiled images to be run predictor on \n",
    "eval_file_lst = []\n",
    "eval_file_lst = eval_file_lst + glob.glob('./data/crop/*.JPEG')\n",
    "\n",
    "# Create detectron2 config and predictor \n",
    "cfg = get_cfg()\n",
    "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "# download model weights\n",
    "!gdown -q https://drive.google.com/uc?id=1-f_INg5D0yG7AJUkuSJUcIl6BSaf-smR \n",
    "# load model weights \n",
    "cfg.MODEL.WEIGHTS = \"./model_final.pth\"\n",
    "\n",
    "BIRD_SPECIES = [\"Brown Pelican\", \"Laughing Gull\", \"Mixed Tern\",\n",
    "                \"Great Blue Heron\",\"Great Egret/White Morph\"]\n",
    "SPECIES_MAP = {0: 'Brown Pelican', 1: 'Laughing Gull', 2: 'Mixed Tern', 3: 'Great Blue Heron',\n",
    "               4: 'Great Egret/White Morph', 5: 'Other/Unknown'}\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 4\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(BIRD_SPECIES) \n",
    "\n",
    "# Create default predictor to run inference \n",
    "predictor = DefaultPredictor(cfg)\n",
    "RAW_IMG_WIDTH = 8192\n",
    "RAW_IMG_HEIGHT = 5460\n",
    "\n",
    "# Run evaluation \n",
    "output_df = evaluate_full_pipeline(eval_file_lst, predictor, SPECIES_MAP, RAW_IMG_WIDTH, RAW_IMG_HEIGHT,\n",
    "                           CROP_WIDTH, CROP_HEIGHT, SLIDING_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDetAeyFBPtP"
   },
   "source": [
    "### Download annotations as CSV file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrBccu--38Ho"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "output_df.to_csv('output.csv')\n",
    "files.download('output.csv') "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMtFjtOerH5KNQRHS/Mz6hg",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Audubon-Bird-Detection-Tutorial.ipynb",
   "provenance": []
  },
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.8 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/1.8.1-cpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
